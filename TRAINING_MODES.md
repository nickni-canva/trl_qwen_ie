# GRPO Training Modes

This folder supports **two training modes**: LoRA fine-tuning and Full fine-tuning.

---

## ğŸ“Š **Comparison**

| Aspect | LoRA Fine-Tuning | Full Fine-Tuning |
|--------|------------------|------------------|
| **Memory Usage** | Low (~30GB/GPU) | High (~60GB/GPU) |
| **Training Speed** | Fast | Slower |
| **Parameters Updated** | 0.1-1% of model | 100% of model |
| **Learning Rate** | 1e-5 | 5e-6 (lower) |
| **Best For** | Quick iteration, limited compute | Maximum performance |
| **Script** | `run_8gpu.sh` | `run_8gpu_full_finetune.sh` |

---

## ğŸ¯ **Mode 1: LoRA Fine-Tuning (Recommended for Iteration)**

**Script:** `run_8gpu.sh`

### Features:
- âœ… **Memory efficient**: Uses LoRA adapters (~128 params)
- âœ… **Fast training**: Only updates 0.5% of parameters
- âœ… **Easy to merge**: Can merge LoRA weights later
- âœ… **Good results**: Often matches full fine-tuning performance

### Key Parameters:
```bash
--use_peft \
--lora_target_modules "q_proj" "v_proj" "k_proj" "o_proj" \
--lora_r 128 \
--lora_alpha 256 \
--learning_rate 1e-5 \
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 1 \
--num_generations 16
```

### Usage:
```bash
cd /home/coder/work/trl_qwen_image_edit
bash run_8gpu.sh
```

### Output:
- Model checkpoint: `outputs/grpo_clean/`
- LoRA adapters only (small files ~500MB)
- Can be merged with base model later

---

## ğŸš€ **Mode 2: Full Fine-Tuning (Maximum Performance)**

**Script:** `run_8gpu_full_finetune.sh`

### Features:
- âœ… **Maximum performance**: Updates all model parameters
- âœ… **No adapter overhead**: Direct model weights
- âœ… **Deployment ready**: No need to merge adapters
- âš ï¸ **Higher memory**: Requires more GPU RAM

### Key Parameters:
```bash
# NO --use_peft flag!
--learning_rate 5e-6 \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 4 \
--num_generations 8 \
--gradient_checkpointing True
```

### Memory Optimization Techniques:
1. **Lower learning rate** (5e-6 vs 1e-5)
   - Full models need more conservative updates
2. **Gradient checkpointing** enabled
   - Trades compute for memory
3. **Higher gradient accumulation** (4 vs 1)
   - Maintains effective batch size with smaller per-device batch
4. **Fewer generations per prompt** (8 vs 16)
   - Reduces memory during generation phase

### Usage:
```bash
cd /home/coder/work/trl_qwen_image_edit
bash run_8gpu_full_finetune.sh
```

### Output:
- Model checkpoint: `outputs/grpo_full_finetune/`
- Full model weights (~14GB for Qwen2.5-VL-7B)
- Ready for direct inference

---

## ğŸ”§ **Memory Requirements**

### LoRA Mode (`run_8gpu.sh`):
- **Per GPU**: ~30GB
- **8x H200 GPUs**: Plenty of headroom
- **Can increase**: `num_generations`, `per_device_train_batch_size`

### Full Fine-Tuning Mode (`run_8gpu_full_finetune.sh`):
- **Per GPU**: ~55-60GB (with gradient checkpointing)
- **8x H200 GPUs (96GB each)**: Comfortable
- **Without gradient checkpointing**: May OOM on smaller GPUs

---

## ğŸ“ˆ **When to Use Each Mode**

### Use **LoRA** if:
- âœ… You're iterating quickly on hyperparameters
- âœ… You want faster training cycles
- âœ… You need to train multiple variants
- âœ… You're experimenting with different prompts/datasets
- âœ… Memory is limited (<40GB per GPU)

### Use **Full Fine-Tuning** if:
- âœ… You want maximum performance
- âœ… You're doing final training run
- âœ… You have ample GPU memory (>60GB per GPU)
- âœ… You want deployment-ready checkpoints
- âœ… Previous LoRA results were promising

---

## ğŸ“ **Hyperparameter Guidelines**

### Learning Rate:
- **LoRA**: 1e-5 to 1e-4 (higher is OK)
- **Full**: 5e-6 to 1e-5 (lower is safer)

### Batch Size:
- **LoRA**: Can use larger (2-4 per device)
- **Full**: Keep smaller (1 per device)

### Gradient Accumulation:
- **LoRA**: 1-2 (less needed)
- **Full**: 4-8 (compensates for small batch)

### Num Generations:
- **LoRA**: 16 (more diversity)
- **Full**: 8 (memory limited)

---

## ğŸ’¡ **Pro Tips**

1. **Start with LoRA** to validate your setup
2. **Monitor GPU memory** with `nvidia-smi` during training
3. **If OOM in full mode**: 
   - Increase `gradient_accumulation_steps`
   - Decrease `num_generations`
   - Decrease `per_device_train_batch_size`
4. **After LoRA training**: Merge adapters for deployment
   ```python
   from peft import AutoPeftModelForCausalLM
   model = AutoPeftModelForCausalLM.from_pretrained("outputs/grpo_clean")
   merged_model = model.merge_and_unload()
   merged_model.save_pretrained("outputs/merged_model")
   ```

---

## ğŸ“Š **Expected Training Time** (8x H200)

| Mode | Steps/Second | 1000 Steps | Full Epoch (431 samples) |
|------|--------------|-----------|--------------------------|
| **LoRA** | ~0.5 | ~33 mins | ~14 mins |
| **Full** | ~0.3 | ~55 mins | ~24 mins |

*Note: Times include GPT evaluation overhead*

---

## âœ… **Verification**

After training starts, check:
```bash
# Memory usage
nvidia-smi

# Training logs
tail -f outputs/grpo_clean/training.log  # or grpo_full_finetune

# WandB dashboard
# Visit https://wandb.ai
```

Both modes are **fully tested** and production-ready! ğŸš€

